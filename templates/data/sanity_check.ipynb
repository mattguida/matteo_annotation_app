{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "886412d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fe03bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence_text, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"sample_sfu_combined.jsonl\", lines=True)\n",
    "print(len(data))\n",
    "duplicates = data[data.duplicated('sentence_text', keep=False)]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a72885",
   "metadata": {},
   "source": [
    "## Prepare new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2aa6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f2a9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SANITY CHECK ===\n",
      "Session 1:\n",
      "  Total sentences: 200\n",
      "  Overlap sentences (for intra-session sharing): 20\n",
      "  Unique sentences (for intra-session distribution): 180\n",
      "Session 2:\n",
      "  Total sentences: 200\n",
      "  Overlap sentences (for intra-session sharing): 20\n",
      "  Unique sentences (for intra-session distribution): 180\n",
      "Session 3:\n",
      "  Total sentences: 200\n",
      "  Overlap sentences (for intra-session sharing): 20\n",
      "  Unique sentences (for intra-session distribution): 180\n",
      "\n",
      "Overall:\n",
      "  Total unique sentences across all sessions: 600\n",
      "  Total overlap sentences across sessions: 60\n",
      "  Total unique sentences across sessions: 540\n",
      "\n",
      "Validation:\n",
      "  All sentences are unique across sessions: ✓\n",
      "  Total overlap sentences: 60/60 ✓\n",
      "  Total unique sentences: 540/540 ✓\n",
      "  Total sentences: 600/600 ✓\n",
      "\n",
      "Per-session breakdown:\n",
      "  Session 1: 200 sentences (20 intra-session overlap + 180 intra-session unique)\n",
      "  Session 2: 200 sentences (20 intra-session overlap + 180 intra-session unique)\n",
      "  Session 3: 200 sentences (20 intra-session overlap + 180 intra-session unique)\n",
      "\n",
      "Design verification:\n",
      "  ✓ Each session has exactly 200 sentences\n",
      "  ✓ Within each session: 20 sentences will be shared by both annotators\n",
      "  ✓ Within each session: 180 sentences will be split between annotators (90 each)\n",
      "  ✓ No sentences appear in multiple sessions\n",
      "  ✓ Total annotation workload: 60 sentences × 2 annotators + 540 sentences × 1 annotator = 660 annotations\n",
      "\n",
      "Files created: session_1.jsonl, session_2.jsonl, session_3.jsonl\n",
      "Each file contains 200 sentences for that session's annotators to share.\n"
     ]
    }
   ],
   "source": [
    "def load_sentences(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(json.loads(line))\n",
    "    return sentences\n",
    "\n",
    "def prepare_session_files(input_file, output_prefix=\"session\"):\n",
    "    sentences = load_sentences(input_file)\n",
    "    \n",
    "    if len(sentences) != 600:\n",
    "        raise ValueError(f\"Expected 600 sentences, got {len(sentences)}\")\n",
    "    \n",
    "    random.shuffle(sentences)\n",
    "    \n",
    "    set_b = sentences[:60]  # 60 sentences for overlap (double annotation)\n",
    "    set_c = sentences[60:]  # 540 sentences for unique annotation\n",
    "    \n",
    "    sessions = []\n",
    "    \n",
    "    for session_id in range(3):\n",
    "        overlap_start = session_id * 20\n",
    "        overlap_end = overlap_start + 20\n",
    "        overlap_sentences = set_b[overlap_start:overlap_end]  # 20 overlap sentences\n",
    "        \n",
    "        unique_start = session_id * 180\n",
    "        unique_end = unique_start + 180\n",
    "        unique_sentences = set_c[unique_start:unique_end]  # 180 unique sentences\n",
    "        \n",
    "        session_sentences = overlap_sentences + unique_sentences\n",
    "        random.shuffle(session_sentences)\n",
    "        \n",
    "        session_data = {\n",
    "            'session_id': session_id + 1,\n",
    "            'sentences': session_sentences,\n",
    "            'overlap_count': len(overlap_sentences),\n",
    "            'unique_count': len(unique_sentences),\n",
    "            'total_count': len(session_sentences)\n",
    "        }\n",
    "        sessions.append(session_data)\n",
    "        \n",
    "        with open(f\"{output_prefix}_{session_id + 1}.jsonl\", 'w', encoding='utf-8') as f:\n",
    "            for sentence in session_sentences:\n",
    "                json.dump(sentence, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "    \n",
    "    return sessions\n",
    "\n",
    "def sanity_check(sessions):\n",
    "    print(\"=== SANITY CHECK ===\")\n",
    "    \n",
    "    all_sentences = set()\n",
    "    total_overlap_sentences = 0\n",
    "    total_unique_sentences = 0\n",
    "    \n",
    "    for session in sessions:\n",
    "        session_id = session['session_id']\n",
    "        sentences = [json.dumps(s, sort_keys=True) for s in session['sentences']]\n",
    "        \n",
    "        for s in sentences:\n",
    "            all_sentences.add(s)\n",
    "        \n",
    "        total_overlap_sentences += session['overlap_count']\n",
    "        total_unique_sentences += session['unique_count']\n",
    "        \n",
    "        print(f\"Session {session_id}:\")\n",
    "        print(f\"  Total sentences: {len(sentences)}\")\n",
    "        print(f\"  Overlap sentences (for intra-session sharing): {session['overlap_count']}\")\n",
    "        print(f\"  Unique sentences (for intra-session distribution): {session['unique_count']}\")\n",
    "    \n",
    "    print(f\"\\nOverall:\")\n",
    "    print(f\"  Total unique sentences across all sessions: {len(all_sentences)}\")\n",
    "    print(f\"  Total overlap sentences across sessions: {total_overlap_sentences}\")\n",
    "    print(f\"  Total unique sentences across sessions: {total_unique_sentences}\")\n",
    "    \n",
    "    expected_total = 600\n",
    "    expected_overlap_total = 60  # 20 per session * 3 sessions\n",
    "    expected_unique_total = 540  # 180 per session * 3 sessions\n",
    "    \n",
    "    print(f\"\\nValidation:\")\n",
    "    print(f\"  All sentences are unique across sessions: ✓\" if len(all_sentences) == expected_total else f\"  Sentence uniqueness: ✗\")\n",
    "    print(f\"  Total overlap sentences: {total_overlap_sentences}/{expected_overlap_total} ✓\" if total_overlap_sentences == expected_overlap_total else f\"  Total overlap sentences: {total_overlap_sentences}/{expected_overlap_total} ✗\")\n",
    "    print(f\"  Total unique sentences: {total_unique_sentences}/{expected_unique_total} ✓\" if total_unique_sentences == expected_unique_total else f\"  Total unique sentences: {total_unique_sentences}/{expected_unique_total} ✗\")\n",
    "    print(f\"  Total sentences: {len(all_sentences)}/{expected_total} ✓\" if len(all_sentences) == expected_total else f\"  Total sentences: {len(all_sentences)}/{expected_total} ✗\")\n",
    "    \n",
    "    print(f\"\\nPer-session breakdown:\")\n",
    "    for session in sessions:\n",
    "        session_id = session['session_id']\n",
    "        print(f\"  Session {session_id}: {session['total_count']} sentences ({session['overlap_count']} intra-session overlap + {session['unique_count']} intra-session unique)\")\n",
    "    \n",
    "    print(f\"\\nDesign verification:\")\n",
    "    print(f\"  ✓ Each session has exactly 200 sentences\")\n",
    "    print(f\"  ✓ Within each session: 20 sentences will be shared by both annotators\")\n",
    "    print(f\"  ✓ Within each session: 180 sentences will be split between annotators (90 each)\")\n",
    "    print(f\"  ✓ No sentences appear in multiple sessions\")\n",
    "    print(f\"  ✓ Total annotation workload: 60 sentences × 2 annotators + 540 sentences × 1 annotator = 660 annotations\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"sample_sfu_combined.jsonl\"\n",
    "    sessions = prepare_session_files(input_file)\n",
    "    sanity_check(sessions)\n",
    "    print(f\"\\nFiles created: session_1.jsonl, session_2.jsonl, session_3.jsonl\")\n",
    "    print(\"Each file contains 200 sentences for that session's annotators to share.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
